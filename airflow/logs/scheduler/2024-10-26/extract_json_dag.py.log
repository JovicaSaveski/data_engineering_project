[2024-10-26T10:33:08.930+0000] {processor.py:186} INFO - Started process (PID=5063) to work on /opt/airflow/dags/extract_json_dag.py
[2024-10-26T10:33:08.931+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/extract_json_dag.py for tasks to queue
[2024-10-26T10:33:08.935+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:08.935+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/extract_json_dag.py
[2024-10-26T10:33:08.980+0000] {processor.py:925} INFO - DAG(s) 'extract_json_dag' retrieved from /opt/airflow/dags/extract_json_dag.py
[2024-10-26T10:33:09.166+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.165+0000] {override.py:1900} INFO - Created Permission View: can read on DAG:extract_json_dag
[2024-10-26T10:33:09.183+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.183+0000] {override.py:1900} INFO - Created Permission View: can edit on DAG:extract_json_dag
[2024-10-26T10:33:09.194+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.194+0000] {override.py:1900} INFO - Created Permission View: can delete on DAG:extract_json_dag
[2024-10-26T10:33:09.208+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.208+0000] {override.py:1900} INFO - Created Permission View: can read on DAG Run:extract_json_dag
[2024-10-26T10:33:09.219+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.219+0000] {override.py:1900} INFO - Created Permission View: can delete on DAG Run:extract_json_dag
[2024-10-26T10:33:09.231+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.230+0000] {override.py:1900} INFO - Created Permission View: menu access on DAG Run:extract_json_dag
[2024-10-26T10:33:09.243+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.243+0000] {override.py:1903} ERROR - Creation of Permission View Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "ab_permission_view_permission_id_view_menu_id_uq"
DETAIL:  Key (permission_id, view_menu_id)=(3, 205) already exists.

[SQL: INSERT INTO ab_permission_view (permission_id, view_menu_id) VALUES (%(permission_id)s, %(view_menu_id)s) RETURNING ab_permission_view.id]
[parameters: {'permission_id': 3, 'view_menu_id': 205}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-10-26T10:33:09.244+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.244+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-10-26T10:33:09.265+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.265+0000] {dag.py:3262} INFO - Creating ORM DAG for extract_json_dag
[2024-10-26T10:33:09.284+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.284+0000] {dag.py:4180} INFO - Setting next_dagrun for extract_json_dag to 2024-10-25 00:00:00+00:00, run_after=2024-10-26 00:00:00+00:00
[2024-10-26T10:33:09.513+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.504+0000] {dagbag.py:698} ERROR - Failed to write serialized DAG: /opt/airflow/dags/extract_json_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 686, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(extract_json_dag) already exists.

[SQL: INSERT INTO dag (dag_id, root_dag_id, is_paused, is_subdag, is_active, last_parsed_time, last_pickled, last_expired, scheduler_lock, pickle_id, fileloc, processor_subdir, owners, dag_display_name, description, default_view, schedule_interval, timetable_description, dataset_expression, max_active_tasks, max_active_runs, max_consecutive_failed_dag_runs, has_task_concurrency_limits, has_import_errors, next_dagrun, next_dagrun_data_interval_start, next_dagrun_data_interval_end, next_dagrun_create_after) VALUES (%(dag_id)s, %(root_dag_id)s, %(is_paused)s, %(is_subdag)s, %(is_active)s, %(last_parsed_time)s, %(last_pickled)s, %(last_expired)s, %(scheduler_lock)s, %(pickle_id)s, %(fileloc)s, %(processor_subdir)s, %(owners)s, %(dag_display_name)s, %(description)s, %(default_view)s, %(schedule_interval)s, %(timetable_description)s, %(dataset_expression)s, %(max_active_tasks)s, %(max_active_runs)s, %(max_consecutive_failed_dag_runs)s, %(has_task_concurrency_limits)s, %(has_import_errors)s, %(next_dagrun)s, %(next_dagrun_data_interval_start)s, %(next_dagrun_data_interval_end)s, %(next_dagrun_create_after)s)]
[parameters: {'dag_id': 'extract_json_dag', 'root_dag_id': None, 'is_paused': True, 'is_subdag': False, 'is_active': True, 'last_parsed_time': datetime.datetime(2024, 10, 26, 10, 33, 9, 280972, tzinfo=Timezone('UTC')), 'last_pickled': None, 'last_expired': None, 'scheduler_lock': None, 'pickle_id': None, 'fileloc': '/opt/airflow/dags/extract_json_dag.py', 'processor_subdir': '/opt/airflow/dags', 'owners': 'airflow', 'dag_display_name': None, 'description': 'DAG to extract data from json', 'default_view': 'grid', 'schedule_interval': '"@daily"', 'timetable_description': 'At 00:00', 'dataset_expression': 'null', 'max_active_tasks': 16, 'max_active_runs': 16, 'max_consecutive_failed_dag_runs': 0, 'has_task_concurrency_limits': False, 'has_import_errors': False, 'next_dagrun': DateTime(2024, 10, 25, 0, 0, 0, tzinfo=Timezone('UTC')), 'next_dagrun_data_interval_start': DateTime(2024, 10, 25, 0, 0, 0, tzinfo=Timezone('UTC')), 'next_dagrun_data_interval_end': DateTime(2024, 10, 26, 0, 0, 0, tzinfo=Timezone('UTC')), 'next_dagrun_create_after': DateTime(2024, 10, 26, 0, 0, 0, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-10-26T10:33:09.515+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:09.514+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-10-26T10:33:09.517+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 708, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 724, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3252, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(extract_json_dag) already exists.

[SQL: INSERT INTO dag (dag_id, root_dag_id, is_paused, is_subdag, is_active, last_parsed_time, last_pickled, last_expired, scheduler_lock, pickle_id, fileloc, processor_subdir, owners, dag_display_name, description, default_view, schedule_interval, timetable_description, dataset_expression, max_active_tasks, max_active_runs, max_consecutive_failed_dag_runs, has_task_concurrency_limits, has_import_errors, next_dagrun, next_dagrun_data_interval_start, next_dagrun_data_interval_end, next_dagrun_create_after) VALUES (%(dag_id)s, %(root_dag_id)s, %(is_paused)s, %(is_subdag)s, %(is_active)s, %(last_parsed_time)s, %(last_pickled)s, %(last_expired)s, %(scheduler_lock)s, %(pickle_id)s, %(fileloc)s, %(processor_subdir)s, %(owners)s, %(dag_display_name)s, %(description)s, %(default_view)s, %(schedule_interval)s, %(timetable_description)s, %(dataset_expression)s, %(max_active_tasks)s, %(max_active_runs)s, %(max_consecutive_failed_dag_runs)s, %(has_task_concurrency_limits)s, %(has_import_errors)s, %(next_dagrun)s, %(next_dagrun_data_interval_start)s, %(next_dagrun_data_interval_end)s, %(next_dagrun_create_after)s)]
[parameters: {'dag_id': 'extract_json_dag', 'root_dag_id': None, 'is_paused': True, 'is_subdag': False, 'is_active': True, 'last_parsed_time': datetime.datetime(2024, 10, 26, 10, 33, 9, 280972, tzinfo=Timezone('UTC')), 'last_pickled': None, 'last_expired': None, 'scheduler_lock': None, 'pickle_id': None, 'fileloc': '/opt/airflow/dags/extract_json_dag.py', 'processor_subdir': '/opt/airflow/dags', 'owners': 'airflow', 'dag_display_name': None, 'description': 'DAG to extract data from json', 'default_view': 'grid', 'schedule_interval': '"@daily"', 'timetable_description': 'At 00:00', 'dataset_expression': 'null', 'max_active_tasks': 16, 'max_active_runs': 16, 'max_consecutive_failed_dag_runs': 0, 'has_task_concurrency_limits': False, 'has_import_errors': False, 'next_dagrun': DateTime(2024, 10, 25, 0, 0, 0, tzinfo=Timezone('UTC')), 'next_dagrun_data_interval_start': DateTime(2024, 10, 25, 0, 0, 0, tzinfo=Timezone('UTC')), 'next_dagrun_data_interval_end': DateTime(2024, 10, 26, 0, 0, 0, tzinfo=Timezone('UTC')), 'next_dagrun_create_after': DateTime(2024, 10, 26, 0, 0, 0, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-10-26T10:33:40.126+0000] {processor.py:186} INFO - Started process (PID=5129) to work on /opt/airflow/dags/extract_json_dag.py
[2024-10-26T10:33:40.127+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/extract_json_dag.py for tasks to queue
[2024-10-26T10:33:40.132+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:40.131+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/extract_json_dag.py
[2024-10-26T10:33:40.165+0000] {processor.py:925} INFO - DAG(s) 'extract_json_dag' retrieved from /opt/airflow/dags/extract_json_dag.py
[2024-10-26T10:33:40.195+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:40.195+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-10-26T10:33:40.233+0000] {logging_mixin.py:190} INFO - [2024-10-26T10:33:40.233+0000] {dag.py:4180} INFO - Setting next_dagrun for extract_json_dag to 2024-10-25 00:00:00+00:00, run_after=2024-10-26 00:00:00+00:00
[2024-10-26T10:33:40.293+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/extract_json_dag.py took 0.181 seconds
